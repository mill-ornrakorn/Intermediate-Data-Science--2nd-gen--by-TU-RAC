{"cells":[{"cell_type":"markdown","metadata":{"id":"xwkMtbGrtHyd"},"source":["## Hands-On Data Preprocessing in Python\n","Learn how to effectively prepare data for successful data analytics\n","\n","## Data Transformation and Massaging\n","Data transformation comes at the very last stage of data preprocessing, right before using the analytic tools. At this stage of data preprocessing, the dataset already has the following characteristics.\n","- **Data cleaning**: The dataset is cleaned at all three cleaning levels.\n","- **Data integration**: All the potentially beneficial data sources are recognized and a dataset that includes the necessary information is created.\n","- **Data reduction**: If needed, the size of the dataset has been reduced"]},{"cell_type":"markdown","source":["At this stage of data preprocessing, we may have to make some changes to the data before moving to the analyzing stage. The dataset will undergo the changes for one of the following reasons:\n","- **Necessity**: The analytic method cannot work with the current state of the data. For instance, many data-mining algorithms, such as Multi-Layered Perceptron (MLP) and K-means, only work with numbers; when there are categorical attributes, those attributes need to be transformed before the analysis is possible.\n","- **Correctness**: Without the proper data transformation, the resulting analytic will be misleading and wrong. For instance, if we use K-means clustering without normalizing the data, we think that all the attributes have equal weights in the clustering result, but that's incorrect; the attributes that happen to have a larger scale will have more weight.\n","- **Effectiveness**: If the data goes through some prescribed changes, the analytics will be more effective."],"metadata":{"id":"2AA9WUtPvkqu"}},{"cell_type":"markdown","source":["## Data Transformation versus Data Massaging\n","**Data transformation** refers to the process of converting data from one format or structure to another, in order to make it more useful or usable for a specific purpose. This can include things like cleaning and formatting data, as well as aggregating and summarizing it.\n","\n","**Data massaging** is a less formal term that is often used to refer to the process of manipulating data in order to clean it, correct errors, or make it more useful. This can include things like removing duplicate records, filling in missing values, or converting data from one format to another.\n","\n","Both data transformation and data massaging involve manipulating data in order to make it more useful or usable, but the specific techniques and processes used may differ depending on the context and the specific data being worked with.\n","\n","<img src=\"https://drive.google.com/uc?id=1fubPkuIjBhZ3xLIaL4z4uTv6lETrSCwq\" width=\"400\"/>"],"metadata":{"id":"5patnOqkwyda"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNaDcccwtHyg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"G6u1wdCatZiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Normalization and Standardization\n","Here is the general rule of when we need normalization or standardization. We need **normalization** when we need the range of all the attributes in a dataset to be equal. This will be needed especially for algorithmic data analytics that uses the distance between the data objects. Examples of such algorithms are K-means and KNN.\n","\n","$NA_i = \\cfrac{A_i - min(A)}{max(A) - min(A)}$\n","\n","On the other hand, we need **standardization** when we need the variance and/or the standard deviation of all the attributes to be equal. We saw an example of needing standardization when learning about PCA, Data Reduction. We learned standardization was necessary because PCA essentially operates by examining the total variations in a dataset; when an attribute has more variations, it will have more say in the operation of PCA.\n","\n","$SA_i = \\cfrac{A_i - mean(A)}{std(A)}$"],"metadata":{"id":"E958HurQyWGM"}},{"cell_type":"markdown","metadata":{"id":"qxrZp021tHyj"},"source":["## Binary Coding, Ranking Transformation, and Discretization\n","\n","<img src=\"https://drive.google.com/uc?id=1xfVckidbD7pTyNXLbShyOfrV-x6tRuAp\" width=\"400\"/>"]},{"cell_type":"markdown","source":["### Example 1 â€“ Binary coding of nominal attribute\n","---\n","In this example, we will use the **WH Report.csv** dataset. This dataset contains the **Continent** categorical attribute. This attribute indeed has information that can add to the interestingness of our clustering analysis."],"metadata":{"id":"9iv2-a94hrFP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOP42gcLtHyk"},"outputs":[],"source":["report_df = pd.read_csv('WH Report.csv')\n","BM = report_df.year == 2019\n","\n","report2019_df = report_df[BM]\n","report2019_df.set_index('Name',inplace=True)"]},{"cell_type":"markdown","source":["As the attribute continent is nominal, we only have one choice and that is to use binary coding. In the following code, we will use the **pd.get_dummies()** pandas function to binary-code the Continent attribute."],"metadata":{"id":"pCSdg6rokDmJ"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"AHrfF61HtHyk"},"outputs":[],"source":["bc_Continent = pd.get_dummies(report2019_df.Continent)\n","bc_Continent.head(5)"]},{"cell_type":"markdown","source":["The addition of **Xs = Xs.join(bc_Continent/7)**, which adds the binary coded version of the Continent attribute (bc_Continent) to Xs after Xs is **normalized**, and before it is fed into **kmeans.fit()**."],"metadata":{"id":"Exzz6GZSr01k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qa55p52WtHym"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","dimensions = ['Life_Ladder', 'Log_GDP_per_capita', 'Social_support',\n","              'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n","              'Generosity', 'Perceptions_of_corruption', 'Positive_affect', 'Negative_affect']\n","\n","Xs = report2019_df[dimensions]\n","Xs = (Xs - Xs.min())/(Xs.max()-Xs.min())\n","Xs = Xs.join(bc_Continent/7)\n","kmeans = KMeans(n_clusters=3)\n","kmeans.fit(Xs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBNtq_yMtHyn"},"outputs":[],"source":["clusters = ['Cluster {}'.format(i) for i in range(3)]\n","\n","Centroids = pd.DataFrame(0.0, index= clusters, columns=Xs.columns)\n","for i,clst in enumerate(clusters):\n","    BM = kmeans.labels_ == i\n","    Centroids.loc[clst] = Xs[BM].mean(axis=0)\n","\n","plt.figure(figsize=(10,4))\n","plt.subplot(1,2,1)\n","sns.heatmap(Centroids[dimensions], linewidths=.5, annot=True, cmap='binary')\n","plt.subplot(1,2,2)\n","sns.heatmap(Centroids[bc_Continent.columns], linewidths=.5, annot=True, cmap='binary')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8lqZkbBMtHyn"},"source":["ðŸ“Œ To see this impact, **remove the division by 7** run the clustering analysis, and create the heatmap of the centroid analysis to see this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9hz25sKtHyn"},"outputs":[],"source":["dimensions = ['Life_Ladder', 'Log_GDP_per_capita', 'Social_support',\n","              'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n","              'Generosity', 'Perceptions_of_corruption', 'Positive_affect', 'Negative_affect']\n","\n","Xs = report2019_df[dimensions]\n","Xs = (Xs - Xs.min())/(Xs.max()-Xs.min())\n","Xs = Xs.join(bc_Continent)\n","kmeans = KMeans(n_clusters=3)\n","kmeans.fit(Xs)\n","\n","clusters = ['Cluster {}'.format(i) for i in range(3)]\n","\n","Centroids = pd.DataFrame(0.0, index=clusters, columns=Xs.columns)\n","\n","for i,clst in enumerate(clusters):\n","    BM = kmeans.labels_==i\n","    Centroids.loc[clst] = Xs[BM].mean(axis=0)\n","\n","plt.figure(figsize=(10,4))\n","plt.subplot(1,2,1)\n","sns.heatmap(Centroids[dimensions], linewidths=.5, annot=True, cmap='binary')\n","plt.subplot(1,2,2)\n","sns.heatmap(Centroids[bc_Continent.columns], linewidths=.5, annot=True, cmap='binary')\n","plt.show()"]},{"cell_type":"markdown","source":["### Example 2 â€“ Binary coding or ranking transformation of ordinal attributes\n","---\n","<img src=\"https://drive.google.com/uc?id=1gFsSq7zRP2sqrlR2j8DQ9MrscbArkNuK\" width=\"600\"/>\n","\n","In the case of **Binary Coding**, the transformation has not assumed any information into the result, but the transformation has stripped the attribute from its ordinal information. You see, if we were to use the binary-coded values instead of the original attribute in our analysis, the data does not show the order of the possible values of the attribute. <u>For example</u>, while the binary-coded values make a distinction between High School and Bachelor, the data does not show that Bachelor comes after High School, as we know it does.\n","\n","**Ranking Transformation**, does not have this shortcoming; however, it has other cons. You see, by trying to make sure that the order of the possible values is maintained, we had to engage numbers by ranking transformation; however, this goes a little bit overboard. By engaging numbers, not only have we successfully included order in between the possible values of the attribute but we have also collaterally assumed information that does not exist in the original attribute. <u>For example</u>, with the ranking transformed attribute, we are assuming there is one unit difference between Bachelors and High School.\n","\n","**Attribute Construction**, which is only possible if we have a good understanding of the attribute. What Attribute Construction tries to fix is the gross assumptions that are added by Ranking Transformation; instead, Attribute Construction uses the knowledge about the original attribute to assume more accurate information into the transformed data. <u>For example</u>, as we know, achieving any of the degrees in the Education Level attribute takes a different number of years of education. So, instead, Attribute Construction uses that knowledge to assume more accurate assumptions about the transformed data."],"metadata":{"id":"n9p-VQfawAII"}},{"cell_type":"markdown","metadata":{"id":"p1hX7Y6NtHyo"},"source":["### Example 3 â€“ Discretization of Numerical attributes\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4zEnJXYtHyo"},"outputs":[],"source":["adult_df = pd.read_csv('Adult.csv')\n","adult_df"]},{"cell_type":"markdown","source":["The box plot that shows the interaction between three attributes, **sex**, **income**, and **hoursPerWeek**, from *adult_df* (**adult.csv**). We had to use a box plot because hoursPerWeek is a numerical attribute."],"metadata":{"id":"Z0pK4DAm47J7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsctGLKXtHyo"},"outputs":[],"source":["plt.figure(figsize=(5,4))\n","sns.boxplot(data=adult_df, y='sex', x='hoursPerWeek', hue='income')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHULz82ZtHyp"},"outputs":[],"source":["plt.figure(figsize=(4,3))\n","adult_df.hoursPerWeek.plot.hist()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIEmQFRotHyp"},"outputs":[],"source":["adult_df['discretized_hoursPerWeek'] = adult_df.hoursPerWeek.apply(lambda v: '>40' if v>40 else ('40' if v==40 else '<40'))"]},{"cell_type":"markdown","source":["The bar chart that has the interaction with the same three attributes, except that the hoursPerWeek numerical attribute has been **discretized**. You can see the magic that the discretization of this attribute has done for us. The bar chart tells the story of the data far better than the box plot."],"metadata":{"id":"XwVc4aDf5qXr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPFGzYYZtHyq"},"outputs":[],"source":["adult_df.groupby(['sex','income']).discretized_hoursPerWeek.value_counts().unstack()[['<40','40', '>40']].plot.barh()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lbfardTrtHyq"},"source":["## Types of Discretization\n","While the best tool to guide us through finding the best way to discretize an attribute is a histogram, there are a few different approaches one might adopt. These approaches are called **equal width**, **equal frequency**, and **ad hoc**.\n","\n","1. The **equal width** approach makes sure that cut-off points will lead to equal intervals of the numerical attribute. For instance, the following screenshot shows the application of the *pd.cut()* function to create 5 equal-width bins from *adult_df.age*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2KaRXiJtHyq"},"outputs":[],"source":["plt.figure(figsize=(4,3))\n","pd.cut(adult_df.age, bins=5).value_counts().sort_index().plot.bar()\n","plt.show()"]},{"cell_type":"markdown","source":["2. The **equal frequency** approach aims to have an equal number of data objects in each bin. For instance, the following screenshot shows the application of the *pd.qcut()* function to create 5 equal-frequency bins from *adult_df.age*."],"metadata":{"id":"tR8jyuIM7EpD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oqI1PmQtHyr"},"outputs":[],"source":["plt.figure(figsize=(4,3))\n","pd.qcut(adult_df.age, q=5, duplicates='drop').value_counts().sort_index().plot.bar()\n","plt.show()"]},{"cell_type":"markdown","source":["3. The **ad hoc** approach prescribes the whereabouts of **cut-off** points based on the numerical attribute and other circumstantial knowledge about the attribute. For instance, we decided to cut adult_df.hoursePerWeek in Example 3 â€“ discretization of numerical attributes ad hoc after having consulted the histogram of the attribute and the circumstantial knowledge that most employees work 40 hours a week in the US."],"metadata":{"id":"QTovfXN87qgQ"}},{"cell_type":"markdown","metadata":{"id":"CnM42dkFtHyr"},"source":["## Attribute Construction\n","### Example 1 â€“ Construct one transformed attribute from two attributes\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfGg7T9DtHyr"},"outputs":[],"source":["person_df = pd.read_csv('500 Person.csv')\n","person_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4GXI0d9tHyr"},"outputs":[],"source":["person_df.Index = person_df.Index.replace({0:'Extremely Weak', 1: 'Weak',2: 'Normal',3:'Overweight', 4:'Obesity',5:'Extreme Obesity'})\n","person_df.columns = ['Gender', 'Height', 'Weight', 'Condition']\n","person_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnIv-EzbtHyr"},"outputs":[],"source":["plt.figure(figsize=(4,3))\n","sns.scatterplot(data=person_df, x='Height', y='Weight', hue='Condition', style='Gender')\n","plt.legend(bbox_to_anchor=(1.05, 1))\n","plt.show()"]},{"cell_type":"markdown","source":["**BMI** is a function that factors in both weight and height to create a healthiness index. The formula is as follows. Be careful â€“ in this formula, weight is in kilograms and height is in meters.\n","\n","$BMI = \\frac{Weight}{Height^2}$"],"metadata":{"id":"JKs6qQXc961W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqYX0sV_tHyr"},"outputs":[],"source":["person_df['BMI'] = person_df.apply(lambda r:r.Weight/((r.Height/100)**2),axis=1)\n","person_df"]},{"cell_type":"markdown","source":["**random.<font color='red'>random</font>(*size=None*)**\n","\n","Return random floats in the half-open interval [0.0, 1.0). Alias for random_sample to ease forward-porting to the new random API."],"metadata":{"id":"l5lJyset_bTt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QoVJmWmtHys"},"outputs":[],"source":["person_df['Random'] = np.random.random(len(person_df))\n","\n","plt.figure(figsize=(9,2))\n","sns.scatterplot(data=person_df, x='BMI',y='Random', hue='Condition')\n","\n","plt.ylim([-0.25,1.25])\n","plt.xticks(np.linspace(10,80,15))\n","plt.yticks([])\n","plt.grid()\n","plt.legend(bbox_to_anchor=(1.01, 1))\n","plt.show()"]},{"cell_type":"markdown","source":["## Feature Extraction\n","\n","<img src=\"https://drive.google.com/uc?id=1GQbJpVlcnwmcBeDhPiifpTN4R7X0tJ1d\" width=\"500\"/>"],"metadata":{"id":"a6Lf93YtBOvL"}},{"cell_type":"markdown","metadata":{"id":"cuLmYVi9tHys"},"source":["## Log Transformation\n","Attributes with **exponential growth or decline** may be problematic for data visualization and clustering analysis; furthermore, they can be problematic for some prediction and classification algorithms where the method uses the distance between the data objects, such as KNN, or where the method drives its performance based on collective performance metrics, such as linear regression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsMu-tILtHyt"},"outputs":[],"source":["country_df = pd.read_csv('GDP 2019 2020.csv')\n","country_df.set_index('Country Name',inplace=True)\n","country_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UopcCostHyt"},"outputs":[],"source":["n_countries = len(country_df)\n","intervals = [i*2 for i in range(75)]\n","\n","wdf = country_df[['2019','2020']].sort_values('2020')\n","wdf['2020'].plot(figsize=(13,3))\n","\n","plt.xticks(intervals,wdf.iloc[intervals].index,rotation=90)\n","plt.show()\n","\n","wdf['2020'].plot.box(vert=False,figsize=(13,1))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"jlM6qhrFtHyt"},"outputs":[],"source":["n_countries = len(country_df)\n","intervals = [i*2 for i in range(75)]\n","\n","wdf = country_df[['2019','2020']].sort_values('2020')\n","wdf['2020'].plot(figsize=(13,3),logy=True)\n","\n","plt.xticks(intervals,wdf.iloc[intervals].index,rotation=90)\n","plt.show()\n","\n","wdf['2020'].plot.box(vert=False,figsize=(13,1),logx=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NUSbYc34tHyu"},"source":["## Smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"UZuUG6B1tHyu"},"outputs":[],"source":["signal_df = pd.read_csv('Noise Data.csv')\n","signal_df"]},{"cell_type":"code","source":["signal_df.drop(columns='t',inplace=True)"],"metadata":{"id":"z-lfBi7vE8Wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["signal_df.Signal.plot(figsize=(10,3))\n","plt.show()"],"metadata":{"id":"2l7WqD6oEywI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we used **Functional Data Analysis (FDA)** to reduce the size of the data, we were interested in replacing the data with the parameters of the function that simulate the data well. However, when smoothing, we want our data with the same size, but we want to remove the noise. In other words, regarding how FDA is applied, it is very similar to both data reduction and smoothing; however, the output of FDA is different for each purpose.\n","\n","> For **smoothing**, we expect to have the same size data as the output, whereas for **data reduction**, we expect to only have the parameters of the fitting function.\n","\n","There are many functions and modules in the space of the Python data analysis environment that use FDA to smooth data. A few of them are savgol_filter from scipy.signal; CubicSpline, UnivariateSpline, splrep, and splev from scipy.Interpolate; and KernelReg from *statsmodels.nonparametric.kernel_regression*.\n","\n","However, none of these functions works as well as it should, and\n","I believe there is much more room for the improvement of smoothing tools in the space of Python data analytics. For instance, the following figure shows the performance of the .KernelReg() function on part of the data (50 numbers) versus its performance on the whole Noise Data.csv file (200 numbers)."],"metadata":{"id":"HSDfkxYLGHya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGQXzq_RtHyu"},"outputs":[],"source":["from statsmodels.nonparametric.kernel_regression import KernelReg\n","\n","x = np.linspace(0,50,50)\n","y = signal_df.Signal.iloc[:50]\n","\n","plt.figure(figsize=(5,3))\n","plt.plot(x, y, '+')\n","\n","kr = KernelReg(y,x,'c')\n","y_pred, y_std = kr.fit(x)\n","\n","plt.plot(x, y_pred)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UcIroD5tHy0"},"outputs":[],"source":["from statsmodels.nonparametric.kernel_regression import KernelReg\n","\n","x = np.linspace(0,200,200)\n","y = signal_df.Signal\n","\n","plt.figure(figsize=(5,3))\n","plt.plot(x, y, '+')\n","\n","kr = KernelReg(y,x,'c')\n","y_pred, y_std = kr.fit(x)\n","\n","plt.plot(x, y_pred)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8__p62BptHy0"},"source":["## Rolling Data Smoothing\n","The biggest difference between **functional data smoothing** and **rolling data smoothing** is that functional data smoothing looks at the whole data as one piece and then tries to find the function that fits the data. In contrast, rolling data smoothing works on incremental windows of the data. The following figure shows what rolling calculation and the incremental windows are using in the first 10 rows of *singnal_df*.\n","\n","<img src=\"https://drive.google.com/uc?id=16bV5VmJ7Ovm1YocxCuAW3KDAYFtSMubI\" width=\"600\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm1fUZN3tHy0"},"outputs":[],"source":["signal_df.Signal.iloc[:10].plot(figsize=(10,3))\n","\n","plt.xticks([i for i in range(10)])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"eGAk-TIwtHy1"},"outputs":[],"source":["signal_df.Signal.plot(figsize=(10,3),label='Signal')\n","signal_df.Signal.rolling(window =5).mean().plot(label='Moving Average Smoothed')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["The first four values for Moving Average Smoothed are NaN, right? It is due to the nature of rolling window calculations. Always, when the width of windows is k, the first k-1 rows will have NaN."],"metadata":{"id":"IGUKfMlsLk3e"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"zkKMiA9LtHy1"},"outputs":[],"source":["pd.DataFrame({'Signal':signal_df.Signal.iloc[:50],\n","              'Moving Average Smoothed':signal_df.Signal.iloc[:50].rolling(window=5).mean()}).head(10)"]},{"cell_type":"markdown","metadata":{"id":"yOQazdDDtHy1"},"source":["## Binning\n","When the process is done to transform a numerical attribute to a categorical one, it is referred to as discretization, and when it is used as a way to combat noise in numerical data, we call the same data transformation binning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDq2BkDQtHy2"},"outputs":[],"source":["plt.figure(figsize=(5,3))\n","adult_df.age.value_counts().sort_index().plot.bar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGGGq2R8tHy2"},"outputs":[],"source":["plt.figure(figsize=(5,3))\n","\n","adult_df['age_binned']=pd.cut(adult_df.age,10)\n","adult_df.age_binned.value_counts().sort_index().plot.bar()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}