{"cells":[{"cell_type":"markdown","metadata":{"id":"E_EbjRAVQe8k"},"source":["## Hands-On Data Preprocessing in Python\n","Learn how to effectively prepare data for successful data analytics\n","\n","## Data Cleaning Level Ⅱ - Unpack, restructure, and reformulate the table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YekmWh7zQe8l"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IAEIX42ZSE9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv4H8n9wQe8m"},"source":["### Example 2.1 – unpacking columns & reformulating the table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nka2ozxRQe8n"},"outputs":[],"source":["from os import listdir\n","directory = 'Speeches'\n","\n","FileNames = listdir(directory)\n","speech_df = pd.DataFrame(index=range(len(FileNames)),columns=['File Name','The Content'])\n","\n","for i,f_name in enumerate(FileNames):\n","  f = open(directory + \"/\" + f_name, \"r\", encoding='utf-8')\n","  f_content = f.readlines()\n","  f.close()\n","\n","  speech_df.at[i,'File Name'] = f_name\n","  speech_df.at[i,'The Content'] = f_content[0]\n","\n","speech_df.columns = ['FileName','Content']"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"IbCguZAbQe8n"},"outputs":[],"source":["speech_df"]},{"cell_type":"markdown","source":["The *FileName* column contains the following information about the speeches in the dataset:\n","*   City: The city where the speech was given\n","*   Date: The date when the speech was given\n","*   Year: The year when the speech was given\n","*   Month: The month when the speech was given\n","*   Day: The day when the speech was given\n"],"metadata":{"id":"JvGrOw2lUrAO"}},{"cell_type":"markdown","metadata":{"id":"3ul3XxRuQe8o"},"source":["### Unpacking FileName\n","---\n","The following are the steps we need to take for the unpacking process:\n","1. Extract City: Use Month from the **CitynameMonthDD_YYYY**.txt pattern to extract\n","the city. Based on this pattern, everything that comes before Month is Cityname.\n","2. Extract Date: Use the extracted Cityname to extract Date.\n","3. Extract Year, Month, and Day from Date.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHfNiuyvQe8o"},"outputs":[],"source":["Months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Oct','Sep','Nov','Dec']\n","def SeparateCity(v):\n","  for mon in Months:\n","    if (mon in v):\n","      return v[:v.find(mon)]\n","\n","speech_df['City'] = speech_df.FileName.apply(SeparateCity)"]},{"cell_type":"code","source":["speech_df.head()"],"metadata":{"id":"LwWtf-21VgQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gALyQ0OFQe8o"},"outputs":[],"source":["def SeparateDate(r):\n","  return r.FileName[len(r.City):r.FileName.find('.txt')]\n","\n","speech_df['Date'] = speech_df.apply(SeparateDate,axis=1)\n","speech_df.Date = pd.to_datetime(speech_df.Date,format='%b%d_%Y')\n","\n","# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n","# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"]},{"cell_type":"code","source":["speech_df.head()"],"metadata":{"id":"Kqnbs9S6koUs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdO7cq8NQe8p"},"outputs":[],"source":["def extractDMY(r):\n","  r['Day'] = r.Date.day\n","  r['Month'] = r.Date.month\n","  r['Year'] = r.Date.year\n","  return r\n","\n","speech_df = speech_df.apply(extractDMY,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBLPGt9KQe8p"},"outputs":[],"source":["speech_df.drop(columns=['FileName'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7UIXqk9dQe8p"},"outputs":[],"source":["speech_df.head()"]},{"cell_type":"markdown","metadata":{"id":"pyxwCSv0Qe8p"},"source":["### Unpacking Content\n","---\n","The following code creates the *FindWordRatio()* function and applies it to **speech_df**. The function uses a **for** loop to add four new columns to the DataFrame, one column for each of the four words. The calculation for each word is simple: the returning value for each word is the total occurrence of the word in the speech *(row.Content.count(w))*, divided by the total number of words in the speech *(total_n_words)*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zhnoiJ4Qe8q"},"outputs":[],"source":["Words = ['vote','tax','campaign','economy']\n","\n","def FindWordRatio(row):\n","  total_n_words = len(row.Content.split(' '))\n","  for w in Words:\n","    row['r_{}'.format(w)] = row.Content.count(w)/total_n_words\n","  return row\n","\n","speech_df = speech_df.apply(FindWordRatio,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3E5S8YGgQe8q"},"outputs":[],"source":["speech_df.head()"]},{"cell_type":"markdown","metadata":{"id":"koGIEy72Qe8q"},"source":["### Reformulate a new table for visualization\n","---\n","When we need to reformulate a dataset so that its new definition of data objects is an aggregation of the current definition of data objects, we need to perform two steps:\n","1. Create a column that can be the unique identifier for the reformulated dataset.\n","2. Use a function that can reformulate the dataset while applying the aggregate functions. The pandas functions that can do this are <font color='blue'>.groupby()</font> and <font color='blue'>.pivot_table()</font>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynRB4-H6Qe8q"},"outputs":[],"source":["Months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Oct','Sep','Nov','Dec']\n","lambda_func = lambda r: '{}_{}'.format(r.Year,Months[r.Month-1])\n","speech_df['Y_M'] = speech_df.apply(lambda_func,axis=1)"]},{"cell_type":"markdown","source":["> **Lambda functions** are similar to user-defined functions but without a name. They're commonly referred to as anonymous functions. Lambda functions are efficient whenever you want to create a function that will only contain simple expressions – that is, expressions that are usually a single line of a statement. They're also useful when you want to use the function once."],"metadata":{"id":"TDcjGw7Ku6G0"}},{"cell_type":"code","source":["speech_df.head()"],"metadata":{"id":"s8rd8QBwrplJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZT0BWyUQe8q"},"outputs":[],"source":["Words = ['vote','tax','campaign','economy']\n","vis_df = speech_df.pivot_table(\n","    index= ['Y_M'],\n","    values= ['r_{}'.format(w) for w in Words],\n","    aggfunc= np.mean)\n","\n","# https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5CbUpR6Qe8q"},"outputs":[],"source":["vis_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bvR8r9tQe8q"},"outputs":[],"source":["vis_df = pd.DataFrame({\n","  'r_vote': speech_df.groupby('Y_M').r_vote.mean(),\n","  'r_tax': speech_df.groupby('Y_M').r_tax.mean(),\n","  'r_campaign': speech_df.groupby('Y_M').r_campaign.mean(),\n","  'r_economy': speech_df.groupby('Y_M').r_economy.mean()\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01RMY4bxQe8r"},"outputs":[],"source":["vis_df"]},{"cell_type":"markdown","metadata":{"id":"ZS6L29aHQe8r"},"source":["### The last step: draw the visualization\n","---\n","The preceding code creates two lists: **column_order** and **row_order**. As their names suggest, these lists are the order in which the columns and rows will be shown on the visual. The **column_order** is the list of words based on the summation of their occurrence ratio, while **row_order** is the list of *Y_M* based on their natural order in the calendar."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"0rNdkwuVQe8r"},"outputs":[],"source":["column_order = vis_df.sum().sort_values(ascending=False).index\n","row_order = speech_df.sort_values('Date').Y_M.unique()\n","\n","vis_df[column_order].loc[row_order].plot.bar(figsize=(10,4))\n","plt.legend(['vote','tax','campaign','economy'],ncol=2)\n","plt.xlabel('Year_Month')\n","plt.ylabel('Average Word Frequency')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3Hrj3wcxQe8r"},"source":["### Example 2.2 – restructure the table\n","\n","In this example, we will use the **Customer Churn.csv** dataset. This dataset contains the records of 3,150 customers of a telecommunication company. The dataset also specifies whether each customer was churned or not 3 months after the 9 months of collecting the activity data of the customers. Customer churning, from a telecommunication company's point of view, means the customer stops using the company's services and receives the services from the company's competition.\n","\n","We would like to use box plots to compare the two populations of churning customers and non-churning customers for the following activity columns: **Call Failure, Subscription Length, Seconds of Use, Frequency of use, Frequency of SMS, and Distinct Called Numbers.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"V6IA1vnSQe8r"},"outputs":[],"source":["customer_df = pd.read_csv('Customer Churn.csv')\n","customer_df.head(1)"]},{"cell_type":"markdown","source":["> While the column titles are intuitive, they can become more codable. The following line of code makes sure that the columns are also codable:"],"metadata":{"id":"QB_twQOHztuN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pnMGfZ7Qe8r"},"outputs":[],"source":["customer_df.columns = ['Call_Failure', 'Complains', 'Subscription_Length', 'Seconds_of_Use',\n","                       'Frequency_of_use', 'Frequency_of_SMS', 'Distinct_Called_Numbers',\n","                       'Status', 'Churn']"]},{"cell_type":"code","source":["customer_df.head(5)"],"metadata":{"id":"oJbJvAqMzYes"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> For the **box plot** we want to draw, the dictionary we need has two keys – **churn and non-churn** – one for each population that will be presented. The value for each key is the collection of *Call_Failure* records for each population. Pay attention to the fact that, unlike a table data structure that has two dimensions (rows and columns), a dictionary only has one dimension.\n","\n"],"metadata":{"id":"Uvhj00gZ10Ki"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXx-Ke6vQe8r"},"outputs":[],"source":["churn_possibilities = customer_df.Churn.unique()\n","\n","box_sr = pd.Series('',index = churn_possibilities)\n","\n","for poss in churn_possibilities:\n","    BM = customer_df.Churn == poss\n","    box_sr[poss] = customer_df[BM].Call_Failure.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JneNjVTeQe8r"},"outputs":[],"source":["print(box_sr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cg_3aE-CQe8r"},"outputs":[],"source":["plt.figure(figsize=(5,3))\n","\n","plt.boxplot(box_sr,vert=False)\n","plt.yticks([1,2],['Not Churn','Churn'])\n","plt.show()"]},{"cell_type":"markdown","source":["> **Enumerate()** method adds a counter to an iterable and returns it in a form of enumerating object. This enumerated object can then be used directly for loops or converted into a list of tuples using the list() function."],"metadata":{"id":"nDn58EvV6cL1"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"eY6urrelQe8r"},"outputs":[],"source":["select_columns = ['Call_Failure', 'Subscription_Length', 'Seconds_of_Use',\n","                  'Frequency_of_use', 'Frequency_of_SMS', 'Distinct_Called_Numbers']\n","churn_possibilities = customer_df.Churn.unique()\n","\n","plt.figure(figsize=(15,5))\n","for i,sc in enumerate(select_columns):\n","  for poss in churn_possibilities:\n","    BM = customer_df.Churn == poss\n","    box_sr[poss] = customer_df[BM][sc].values\n","  plt.subplot(2,3,i+1)  # subplot(nrows, ncols, index)\n","  plt.boxplot(box_sr,vert=False)\n","  plt.yticks([1,2],['Not Churn','Churn'])\n","  plt.title(sc)\n","\n","plt.tight_layout()  # Adjust the padding between and around subplots.\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"b0683RNyQe8s"},"source":["## Example 2.3 – Data Cleaning Level Ⅰ and Ⅱ\n","\n","In this example, we want to use **Electric Production.csv** to make predictions. We are specifically interested in being able to predict what the monthly electricity demand will be 1 month from now. This 1-month gap is designed in the prediction model so that the predictions that come from the model will have decision-making values; that is, the decision-makers will have time to react to the predicted value.\n","\n","We would like to use **linear regression** to perform this prediction. The independent and dependent attributes for this prediction are shown in the following diagram:\n","\n","<img src=\"https://drive.google.com/uc?id=151tanGbB1N9sa8ldRvMbG76YiFmMacoD\" width=\"500\"/>\n","\n","1.   **Average demand of the month over the years**: For instance, if the month we want to predict demands for is March 2022, we want to use the average of the demands for every March in the previous years. So, we will collate the historical demands of March from the beginning of the data collection process (1985) to 2021 and calculate its average.\n","2.   **Slope of change for the demand of the month over the years**: For instance, if the month we want to predict demands for is March 2022, we want to use the slope of change in the demand in March over the years. As shown in the following diagram, we can fit a line on the Demand in March data points across the years. The slope of that fitted line will be used for prediction.\n","1.   **Average demands of months t-2, t-3, and t-4**: In the preceding diagram, the t, t-2, t-3, and t-4 notations are used to create a time reference. This time reference is that if we want to predict the demand of a month, we want to use the average demand of the following data points: the monthly demand of 2 months ago, the monthly demand of 3 months ago, and the monthly demand of 4 months ago. For instance, if we want to predict the monthly demand of March 2021, we'd want to calculate the average of January 2021, December 2020, and November 2020. Note that we skipped February 2021 as it was our planned decision-making gap.\n","\n","<img src=\"https://drive.google.com/uc?id=1akVGn5q8snz_O4-gpISklstuwzBuVESA\" width=\"500\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRyHbtsbQe8s"},"outputs":[],"source":["month_df = pd.read_csv('Electric Production.csv')\n","month_df"]},{"cell_type":"markdown","metadata":{"id":"WjHsAvlGQe8s"},"source":["### Level Ⅰ cleaning\n","---\n","The **month_df** dataset could do with the following level I data cleaning steps:\n","*   The title of the second column can be more intuitive.\n","*   The data type of the DATE column can be switched to datetime so that we can take advantage of datetime programming properties.\n","*   The default index that's been assigned to the data by pandas can be improved as the DATE column would provide a better and more unique identification.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfuvqtD5Qe8s"},"outputs":[],"source":["month_df.columns = ['Date','Demand']\n","month_df.set_index(pd.to_datetime(month_df.Date,format='%m/%d/%Y'),inplace=True)\n","month_df.drop(columns=['Date'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1-eKWeyQe8s"},"outputs":[],"source":["month_df"]},{"cell_type":"markdown","metadata":{"id":"AMhLTiZJQe8s"},"source":["### Level Ⅱ cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgvUOh3_Qe8s"},"outputs":[],"source":["attributes_dic={'IA1':'Average demand of the month',\n","                'IA2':'Slope of change for the demand of the month',\n","                'IA3': 'Average demands of months t-2, t-3 and t-4',\n","                'DA': 'Demand of month t'}\n","\n","predict_df = pd.DataFrame(index=month_df.iloc[24:].index,columns=attributes_dic.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZBE2XX2Qe8s"},"outputs":[],"source":["predict_df"]},{"cell_type":"markdown","source":["**Why are the first 24 indices not included?** This is due to the second independent attribute: Slope of change for the demand of the month over the years. As the slope of demand change for each month will be needed for the described prediction model, we cannot have a meaningful slope value for the first 24 rows of month_df in predict_df. This is because we at least need two historical data points for each month to be able to calculate a slope for the second independent attribute."],"metadata":{"id":"peS5W1zHF1lI"}},{"cell_type":"markdown","metadata":{"id":"tQEr5Rz-Qe8s"},"source":["### Fill out DA"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"589EFAD3Qe8t"},"outputs":[],"source":["predict_df.DA = month_df.loc['1987-01-01':].Demand\n","predict_df"]},{"cell_type":"markdown","metadata":{"id":"0ZjU_K3iQe8t"},"source":["### Fill out IA1\n","\n","To compute IA1, which is the <font color='blue'>Average demand of the month over the years</font>, we need to be able to filter month_df using the value of the month. To create such a capability, the following code maps a lambda function to month_df and extracts the month of each row:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvr2Lf3wQe8t"},"outputs":[],"source":["month_df['Month'] = list(map(lambda v:v.month, month_df.index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhBkExcJQe8t"},"outputs":[],"source":["month_df"]},{"cell_type":"markdown","source":["The function **ComputeIA1()** that is written to be applied to the rows of *predict_df*, performs the following steps:\n","1. First, it filters out *month_df* using the calculated *row_date* to remove the data points whose dates are after *row_date*.\n","2. Second, the function uses a Boolean mask to keep the data points with the same month as the row's month *(row_date.month)*.\n","3. Next, the function calculates the average demand of the filtered data points and then returns it."],"metadata":{"id":"w9JqMRjBOUCW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"coA1E5dBQe8u"},"outputs":[],"source":["def ComputeIA1(r):\n","  row_date = r.name\n","  wdf = month_df.loc[:row_date].iloc[:-1]\n","  BM = wdf.Month == row_date.month\n","  return wdf[BM].Demand.mean()\n","\n","predict_df.IA1 = predict_df.apply(ComputeIA1,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DC68LW0Qe8u"},"outputs":[],"source":["predict_df"]},{"cell_type":"markdown","metadata":{"id":"RoHnOcwiQe8u"},"source":["### Fill out IA2\n","\n","The **ComputeIA2()** function uses LinearRegression from sklearn.linear_model to find the fitted regression equation and then return the calculated coefficient of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKjWUE3SQe8u"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","def ComputeIA2(r):\n","  row_date = r.name\n","  wdf = month_df.loc[:row_date].iloc[:-1]\n","  BM = wdf.Month == row_date.month\n","\n","  wdf = wdf[BM]\n","  wdf.reset_index(drop=True,inplace=True)\n","  wdf.drop(columns = ['Month'],inplace=True)\n","  wdf['integer'] = range(len(wdf))\n","  wdf['ones'] = 1\n","\n","  lm = LinearRegression()\n","  lm.fit(wdf.drop(columns=['Demand']), wdf.Demand)\n","  return lm.coef_[0]\n","\n","predict_df.IA2 = predict_df.apply(ComputeIA2,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JJlBAKGQe8u"},"outputs":[],"source":["predict_df"]},{"cell_type":"markdown","source":["To understand the way ComputeIA2() finds the slope of change for each cell under predict_df.IA2, see the following screenshot, which shows the code and its output for calculating the slope for one cell under predict_df.IA2. The following screenshot calculates the IA2 value for the row with an index of 2017-10-01:"],"metadata":{"id":"NyQWV3vDT1j3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ohf7USEaQe8u"},"outputs":[],"source":["row_date = '2017-10-01'\n","wdf = month_df.loc[:row_date].iloc[:-1]\n","BM = wdf.Month == 10\n","wdf = wdf[BM]\n","wdf.reset_index(drop=True,inplace=True)\n","wdf.drop(columns = ['Month'],inplace=True)\n","wdf['integer'] = range(len(wdf))\n","wdf['ones'] = 1\n","\n","lm = LinearRegression()\n","lm.fit(wdf.drop(columns=['Demand']), wdf.Demand)\n","print('Slope = {}'.format(lm.coef_[0]))\n","\n","wdf.plot.scatter(x='integer',y='Demand',marker='*',label='Data points',c='C0')\n","\n","b = lm.intercept_\n","a = lm.coef_[0]\n","\n","X = wdf.integer\n","y = b + a*X\n","\n","plt.plot(X,y,label = 'Fitted regression',linestyle='--',c='C1')\n","plt.show()\n","\n","# https://matplotlib.org/stable/tutorials/colors/colors.html"]},{"cell_type":"markdown","metadata":{"id":"WmDTPoxOQe8u"},"source":["### Fill out IA3\n","\n","The following code creates the **ComputeIA3()** function and applies it to *predict_df*. This function uses the index of *predict_df* to find the demand values from 2 months ago, 3 months ago, and 4 months ago.\n","\n","It does this by filtering out all the data that is after *row_date* using <font color='red'>.loc[:row_date]</font>, and then by only keeping the fourth, third, and second rows of the remaining data from the bottom using <font color='red'>.iloc[-5:-2]</font>. Once the data filtering process is complete, the average of three demand values is returned:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_yFBp6jQe8u"},"outputs":[],"source":["def ComputeIA3(r):\n","  row_date = r.name\n","  wdf = month_df.loc[:row_date].iloc[-5:-2]\n","\n","  return wdf.Demand.mean()\n","\n","predict_df.IA3 = predict_df.apply(ComputeIA3,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsVrJ8PGQe8u"},"outputs":[],"source":["predict_df"]},{"cell_type":"markdown","metadata":{"id":"tfO02ZsGQe8u"},"source":["### Doing the analytics – Using Linear Regression to create a predictive model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLXc6AbPQe8v"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","X = predict_df.drop(columns=['DA'])\n","y = predict_df.DA\n","\n","lm = LinearRegression()\n","lm.fit(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujarW13gQe8v"},"outputs":[],"source":["print('intercept (b0) ', lm.intercept_)\n","coef_names = ['b1','b2','b3']\n","print(pd.DataFrame({'Predictor': X.columns,\n","                    'coefficient Name':coef_names,\n","                    'coefficient Value': lm.coef_}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G29HjD4kQe8v"},"outputs":[],"source":["plt.figure(figsize=(10,4))\n","plt.plot(X.index, y, label='Actual')\n","plt.plot(X.index, lm.predict(X), label = 'Fitted', linestyle='--')\n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}