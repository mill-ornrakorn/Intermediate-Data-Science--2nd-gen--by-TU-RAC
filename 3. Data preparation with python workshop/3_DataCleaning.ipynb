{"cells":[{"cell_type":"markdown","metadata":{"id":"z8mNykPSZJen"},"source":["## Hands-On Data Preprocessing in Python\n","Learn how to effectively prepare data for successful data analytics\n","    \n","## Data Cleaning Level Ⅲ – Missing values, outliers, and errors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZDmzAWAZJeq"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qmSguhB2jtDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AB-J2FsKZJer"},"source":["## Missing values\n","\n","Missing values, as the name suggests, are values we expect to have but we don't. In the simplest terms, missing values are empty cells in a dataset that we want to use for analytic goals.\n","\n","In Python, missing values are not presented with emptiness — they are presented via **NaN**, which is short for **Not a Number**. While the literal meaning of Not a Number does not completely capture all the possible situations for which we have missing values, NaN is used in Python whenever we have missing values.\n","\n","Someone who did not know any better may have used some internal agreements to present missing values with an alternative such as MV, None, 99999, and N/A. If missing values are not presented in a standard way, the first step of dealing with them is to rectify that. In such cases, we detect the values that the author of the dataset meant as missing values and replace them with **np.nan**.\n","\n","## Detecting missing values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"2_urGF4kZJes"},"outputs":[],"source":["air_df = pd.read_csv('Air Data.csv')\n","air_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ffk8tF3ZJet"},"outputs":[],"source":["air_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5JYtRikZJet"},"outputs":[],"source":["print('Number of missing values:')\n","for col in air_df.columns:\n","  n_MV = sum(air_df[col].isna())\n","  print('{}:{}'.format(col,n_MV))"]},{"cell_type":"markdown","metadata":{"id":"LOKBSSMxZJet"},"source":["## Causes of missing values\n","\n","*   Human error.\n","*   Respondents may refuse to answer a survey question.\n","*   The person taking the survey does not understand the question.\n","*   The provided value is an obvious error, so it was deleted.\n","*   Not enough time to respond to questions.\n","*   Lost records due to lack of effective database management.\n","*   Intentional deletion and skipping of data collection (probably with fraudulent intent).\n","*   Participant exiting in the middle of the study.\n","*   Third-party tampering with or blocking data collection.\n","*   Missed observations.\n","*   Sensor malfunctions.\n","*   Programing bugs.\n","\n","## Types of missing values\n","\n","1. **Missing Completely at Random (MCAR)** เมื่อ Missing Value เกิดขึ้นแบบ Random หรือเกิดแบบสุ่มทั่วทั้ง Dataset โดยที่ Data ที่ขาดหายไปไม่ได้ขึ้นกับตัวแปรใดตัวแปรหนึ่ง\n","2. **Missing at Random (MAR)** Data ที่ขาดหายไปไม่ได้หายแบบสุ่มทั้ง Dataset แต่มีการขาดหายไปแบบสุ่มในกลุ่มของ Sub-dataset หรือจาก Sample ที่สุ่มมา\n","3. **Not Missing at Random (NMAR)** เป็นกรณณีของ Data ที่ขาดหายไป\n","มีความสัมพันธ์โดยตรงกับข้อมูลที่ทำการเก็บมาโดยตรง เช่น คนทีมีการศึกษาที่ไม่ดีนักก็มักไม่ให้คำตอบด้านการศึกษาว่าจบชั้นไหน Data ในส่วนนี้จำเป็นต้องพิจารณา เพราะเป็นการขาดหายไปที่มีความสัมพันธ์กับตัวแปร\n","\n","อาจกล่าวได้ว่า **NMAR** เป็น **Non-ignorable** คือไม่อาจจะเผิกเฉยได้ ในขณะที่ MCAR และ MAR อาจจะถูกเรียกว่า Ignorable เพราะเป็นการเกิดแบบสุ่ม\n","\n","<font size='2'>*Ref: https://bigdatarpg.com/2022/05/23/missing-values-คืออะไร/*</font>\n","\n","<img src=\"https://drive.google.com/uc?id=1f6r3ckOiFkbadivxv2-PiMssupyCGGoM\" width=\"700\"/>\n","\n","\n"]},{"cell_type":"markdown","source":["## Diagnosis of missing values\n","Diagnosing the missing values in **NO2_Location_A** based on Temperature"],"metadata":{"id":"G0L9zp53rAa1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWAs2j_UZJet"},"outputs":[],"source":["BM_MV = air_df.NO2_Location_A.isna()\n","MV_labels = ['With Missing Values','Without Missing Values']\n","\n","box_sr = pd.Series('',index = BM_MV.unique())\n","\n","for poss in BM_MV.unique():\n","    BM = BM_MV == poss\n","    box_sr[poss] = air_df[BM].Temperature\n","\n","plt.figure(figsize=(5,2))\n","plt.boxplot(box_sr,vert=False)\n","plt.yticks([1,2],MV_labels)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LF-LiLTqZJew"},"source":["## Dealing with missing values\n","1.   Keep them as is.\n","2.   Remove the data objects (rows) with missing values.\n","1.   Remove the attributes (columns) with missing values.\n","2.   Estimate and impute a value.\n","<p style=\"text-indent: 40px\">\n","- Impute with the general central tendency (mean, median, or mode). This is better for MCAR missing values.\n","- Impute with the central tendency of a more relevant group of data to the missing values. This is better for MAR missing values.\n","- Regression analysis. Not ideal, but if we have to proceed with a dataset that has MNAR missing values, this method is better for such a dataset.\n","- Interpolation. When the dataset is a time series dataset and the missing values are of the MCAR type.\n","</p>\n","\n","To effectively find that balance in dealing with missing values, we need to understand and consider the following items:\n","- Our analytic goals\n","- Our analytic tools\n","- The cause of the missing values\n","- The type of the missing values (MCAR, MAR, MNAR)\n","\n","<img src=\"https://drive.google.com/uc?id=1tjXpzx-5HiZCQUDG8ilDs4wV36nNot_m\" width=\"700\"/>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j6IymzHyZJe2"},"source":["## Outliers\n","Outliers, a.k.a. extreme points, are data objects whose values are too different than the rest of the population. Being able to recognize and deal with them is important from the following three perspectives:\n","- Outliers may be data errors in data and should be detected and removed.\n","- Outliers that are not errors can skew the results of analytic tools that are sensitive to the existence of outliers.\n","- Outliers may be fraudulent entries.\n","\n","## Detecting Outliers\n","### (1) Univariate outlier detection\n","The tools we will use for univariate outlier detection depend on the attribute's type. For numerical attributes, we can use a boxplot or the `[Q1-1.5*IQR, Q3+1.5*IQR]` statistical range. The concept of outliers does not have much meaning for a single categorical attribute.\n","\n","<img src=\"https://drive.google.com/uc?id=1bqrDqqT2n1VaWdjEV2ZhGk-3cS9Vg9_a\" width=\"500\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"-fEXJWwWZJe3"},"outputs":[],"source":["response_df = pd.read_csv('Responses.csv')\n","response_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"IIpUTingZJe3"},"source":["#### (1.1) Example of detecting outliers across one numerical attribute"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XI7JP7VYZJe3"},"outputs":[],"source":["plt.figure(figsize=(5,2))\n","plt.boxplot(response_df.Weight.dropna(),vert=False)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"4Cmx1RiYZJe3"},"outputs":[],"source":["Q1 = response_df.Weight.quantile(0.25)\n","Q3 = response_df.Weight.quantile(0.75)\n","IQR = Q3-Q1\n","\n","BM = (response_df.Weight > (Q3+1.5*IQR)) | (response_df.Weight < (Q1-1.5*IQR))\n","response_df[BM]"]},{"cell_type":"markdown","metadata":{"id":"arkOGzKLZJe3"},"source":["#### (1.2) Example of detecting outliers across one categorical attribute"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kjh52fWDZJe3"},"outputs":[],"source":["response_df.Education.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"woTW_QODZJe4"},"outputs":[],"source":["response_df.Education.value_counts().plot.bar(figsize=(5,3))"]},{"cell_type":"markdown","metadata":{"id":"RkzQ6YEAZJe4"},"source":["### (2) Bivariate outlier detection\n","The tools we will use for bivariate outlier detection depend on the attributes' type.\n","- For numerical-numerical attributes, it is best to use a scatterplot.\n","- For categorical-categorical attributes, the tool we use is a color-coded contingency table.\n","- For numerical-categorical attributes, it is best to use multiple boxplots.\n","\n","#### (2.1) Example of detecting outliers across two numerical attributes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-bnmncyZJe4"},"outputs":[],"source":["response_df.plot.scatter(x='Height',y='Weight',figsize=(5,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"PNI44i_1ZJe4"},"outputs":[],"source":["BM = (response_df.Weight>130) | (response_df.Height<70)\n","response_df[BM]"]},{"cell_type":"markdown","metadata":{"id":"ePlJecWaZJe4"},"source":["#### (2.2) Example of detecting outliers across two categorical attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUZv1R89ZJe4"},"outputs":[],"source":["pd.crosstab(response_df['Education'],response_df['God'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdPC7TBoZJe4"},"outputs":[],"source":["cont_table = pd.crosstab(response_df['Education'],response_df['God'])\n","\n","sns.set(rc={\"figure.figsize\":(6, 4)})\n","sns.heatmap(cont_table,annot=True,center=0.5,cmap=\"Greys\")"]},{"cell_type":"markdown","source":["> The **.query()** function, as its name suggests, can also help us perform filtering of a DataFrame based on the values of the attributes."],"metadata":{"id":"rOu5qmhd2T9y"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"N-t8TPQLZJe4"},"outputs":[],"source":["response_df.query('Education== \"currently a primary school pupil\" & God==2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2Rrm1bJZJe5"},"outputs":[],"source":["response_df.query('Education== \"currently a primary school pupil\" & God==4')"]},{"cell_type":"markdown","metadata":{"id":"ZedUwup2ZJe5"},"source":["#### (2.3) Example of detecting outliers across two attributes one categorical and the other numerical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGXjs0L1ZJe5"},"outputs":[],"source":["sns.boxplot(x=response_df.Age,y=response_df.Education)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EZQOV8ZwZJe5"},"outputs":[],"source":["BM1 = (response_df.Education=='college/bachelor degree') & (response_df.Age>26)\n","BM2 = (response_df.Education == 'secondary school') & ((response_df.Age>24) | (response_df.Age<16))\n","BM3 = (response_df.Education == 'primary school') & ((response_df.Age>19) | (response_df.Age<16))\n","BM = BM1 | BM2 | BM3\n","response_df[BM]"]},{"cell_type":"markdown","metadata":{"id":"Vl-T2iuuZJe5"},"source":["### (3) Multivariate Outlier detection\n","Detecting outliers across more than two attributes is called multivariate outlier detection. The best way to go about multivariate outlier detection is through **clustering** analysis.\n","\n","In this example, we would like to see whether we have outliers based on the following four attributes: **Country, Musical, Metal or Hardrock, and Folk**. If you check the complete description of these attributes on *columns_df*, you will realize these attributes describe the liking level of data objects for each of four kinds of music.\n","\n","- First, we will create an Xs attribute, which includes the attributes we want to be used for clustering analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QayaIiR6ZJe6"},"outputs":[],"source":["dimensions = ['Country', 'Metal or Hardrock','Folk','Musical']\n","Xs = response_df[dimensions]"]},{"cell_type":"markdown","source":["- Second, we need to check whether there are any missing values. You may use Xs.info() for the quick detection of missing values."],"metadata":{"id":"LMzVjmJIB-zm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNFCCklwZJe6"},"outputs":[],"source":["Xs.info()"]},{"cell_type":"markdown","source":["- In this case, the missing values are spread across the data objects and the dimensions of Xs. So, we can use the following line of code to impute the missing values with `Q3+IQR*1.5`"],"metadata":{"id":"QYcFak7GCGi0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0tsbwiPZJe6"},"outputs":[],"source":["Q3 = Xs.quantile(0.75)\n","Q1 = Xs.quantile(0.25)\n","IQR = Q3 - Q1\n","Xs = Xs.fillna(Q3+IQR*1.5)"]},{"cell_type":"markdown","source":["- Next, of course, we will not forget to **standardize** the dataset using Xs = (Xs -Xs.min())/(Xs.max()-Xs.min())."],"metadata":{"id":"IpULkUWaCPTP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfwhuUOaZJe6"},"outputs":[],"source":["Xs = (Xs-Xs.min())/(Xs.max()-Xs.min())"]},{"cell_type":"markdown","source":["- Lastly, we can use a loop to perform clustering analysis for different Ks and report its results.\n","- Once the preceding code is successfully run, you can scroll through its prints to see that under none of the Ks, has K-Means grouped one data object or a handful of data objects in one cluster. This will allow us to conclude that there is no multivariate outlier in Xs."],"metadata":{"id":"5FxAh-oqCgEL"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"9KSr-y27ZJe6"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","for k in range(2,8):\n","  kmeans = KMeans(n_clusters=k)\n","  kmeans.fit(Xs)\n","  print('k={}'.format(k))\n","  for i in range(k):\n","    BM = kmeans.labels_==i\n","    print('Cluster {}: {}'.format(i,Xs[BM].index.values))\n","  print('--------- Divider ----------')"]},{"cell_type":"markdown","metadata":{"id":"oGl7rlHGZJe7"},"source":["### (4) Time series outlier detection\n","Outliers in time series data are best detected using **line plots**, the reason being that between consecutive records of a time series there is a close relationship, and using the close relationship is the best way to check the correctness of a record. All you need is to evaluate the value of the record against its closest consecutive records, and that is easily done using line plots."]},{"cell_type":"markdown","metadata":{"id":"KkyP_nFJZJe7"},"source":["## Dealing with outliers\n","1. **Do nothing**\n","2. **Replace with the upper cap or the lower cap**\n","- If the criteria are met, in this approach the outliers are replaced with the correct upper or lower cap.\n","- We replace the univariate outliers that are too much smaller than the rest of the data object with the lower cap of the `Q1-1.5*IQR` attribute, and replace the univariate outliers that are too much larger than the rest of the data objects with the upper cap of the `Q3+1.5*IQR` attribute.\n","3. **Perform log transformation**\n","- This approach is not just a method to deal with outliers but is also an effective data transformation technique. When an attribute follows an exponential distribution, it is only typical for some of the data objects to be very different from the rest of the population. In those situations, applying a log transformation will be the best approach.\n","4. **Remove data objects with outliers**\n","- This is our least favorite approach and should only be used when absolutely necessary. The reason that we would like to avoid this approach is that the data is not incorrect; the values of the outliers are correct but happen to be too different from the rest of the population. It is our analytic tool that is incapable of dealing with the actual population.\n"]},{"cell_type":"markdown","source":["### Example 1\n","---\n","For instance, if we are interested in seeing the frequency changes where most of the population is between 40 and 100, then a histogram without outliers would be better. On the other hand, if a true representation of the population is our end goal, then a histogram with outliers would be ideal."],"metadata":{"id":"zxm5upY8KO_i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-jiO1QHZJe7"},"outputs":[],"source":["plt.figure(figsize=(5,3))\n","response_df.Weight.plot.hist(histtype='step')\n","plt.show()\n","\n","plt.figure(figsize=(5,3))\n","BM = response_df.Weight<105\n","response_df.Weight[BM].plot.hist(histtype='step')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Op4wlXLBZJe8"},"source":["### Example 2\n","---\n","In this example, we would like to use regression to capture the linear relationship between Weight, Height, and Gender to predict Weight. In other words, we would like to find the β0 and β1 values in the following equation: <font color='green'>$Weight = \\beta_{0} + \\beta_{1}×Height + \\beta_{2}×Gender$</font>\n","\n","> Regression analysis is sensitive to outliers."]},{"cell_type":"markdown","metadata":{"id":"X7VRWzECZJe8"},"source":["#### 2.1 Dealing with missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO2KJm9nZJe8"},"outputs":[],"source":["select_attributes = ['Weight','Height','Gender']\n","pre_process_df = pd.DataFrame(response_df[select_attributes])\n","pre_process_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_FaCT_aZJe8"},"outputs":[],"source":["pre_process_df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfL7h34GZJe8"},"outputs":[],"source":["pre_process_df.info()"]},{"cell_type":"markdown","metadata":{"id":"GCxrWVb6ZJe8"},"source":["#### 2.2 Detecting univariate outliers and dealing with them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jr6jc2-yZJe8"},"outputs":[],"source":["num_attributes = ['Weight','Height']\n","for i,att in enumerate(num_attributes):\n","    plt.subplot(1,3,i+1)\n","    pre_process_df[att].plot.box()\n","\n","plt.subplot(1,3,3)\n","pre_process_df.Gender.value_counts().plot.bar()\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","source":["- When the data objects are **univariate outliers**, it is better to **replace them with their statistical upper cap or lower cap**, as replacing the statistical upper or the lower cap will help to keep the data objects and at the same time mitigate the negative effect of the data object with the outliers.\n","\n","- On the other hand — and this also applies generally — when the data objects are **bivariate or multivariate outliers**, it would be better to **remove** them. This is because these outliers will not allow the regression model to capture the patterns among the non-outlier data objects.\n","\n","- In the special case of bivariate outliers whereby the pair of attributes is **categorical-numerical**, it might also be sensible to replace the outlier values with the upper or lower caps of the specific population."],"metadata":{"id":"mlkKgt9GQZss"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpRu0U9EZJe8"},"outputs":[],"source":["Q3 = pre_process_df.Weight.quantile(0.75)\n","Q1 = pre_process_df.Weight.quantile(0.25)\n","IQR = Q3 - Q1\n","\n","upper_cap = Q3+IQR*1.5\n","\n","BM = pre_process_df.Weight > upper_cap\n","pre_process_df.loc[pre_process_df[BM].index,'Weight'] = upper_cap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt-jnd73ZJe9"},"outputs":[],"source":["pre_process_df.Weight.plot.box(figsize=(4,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Msuho-sXZJe9"},"outputs":[],"source":["Q3 = pre_process_df.Height.quantile(0.75)\n","Q1 = pre_process_df.Height.quantile(0.25)\n","IQR = Q3 - Q1\n","\n","lower_cap = Q1-IQR*1.5\n","upper_cap = Q3+IQR*1.5\n","\n","BM = pre_process_df.Height < lower_cap\n","pre_process_df.loc[pre_process_df[BM].index,'Height'] = lower_cap\n","\n","BM = pre_process_df.Height > upper_cap\n","pre_process_df.loc[pre_process_df[BM].index,'Height'] = upper_cap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sv6NsxZMZJe9"},"outputs":[],"source":["pre_process_df.Height.plot.box(figsize=(4,3))"]},{"cell_type":"markdown","metadata":{"id":"1Cs4JYYHZJe9"},"source":["#### 2.3 Detecting bivariate outliers and dealing with them\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqdMWazRZJe9"},"outputs":[],"source":["pre_process_df.plot.scatter(x='Height',y='Weight')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFuc9vsoZJe9"},"outputs":[],"source":["plt.subplot(1,2,1)\n","sns.boxplot(y=pre_process_df.Height,x=pre_process_df.Gender)\n","\n","plt.subplot(1,2,2)\n","sns.boxplot(y=pre_process_df.Weight, x=pre_process_df.Gender)\n","plt.tight_layout()"]},{"cell_type":"markdown","source":["- As these outliers are **bivariate** in a pair of **categorical-numerical** attributes, we may be **replacing** them with the specific population's upper or lower caps."],"metadata":{"id":"3DxgriwdSYXx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3CwMKp8ZJe9"},"outputs":[],"source":["for poss in pre_process_df.Gender.unique():\n","  BM = pre_process_df.Gender == poss\n","  wdf = pre_process_df[BM]\n","  Q3 = wdf.Height.quantile(0.75)\n","  Q1 = wdf.Height.quantile(0.25)\n","  IQR = Q3 - Q1\n","\n","  lower_cap = Q1-IQR*1.5\n","  upper_cap = Q3+IQR*1.5\n","\n","  BM = wdf.Height > upper_cap\n","  pre_process_df.loc[wdf[BM].index,'Height'] = upper_cap\n","\n","  BM = wdf.Height < lower_cap\n","  pre_process_df.loc[wdf[BM].index,'Height'] = lower_cap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9vdFoRQZJe9"},"outputs":[],"source":["for poss in pre_process_df.Gender.unique():\n","  BM = pre_process_df.Gender == poss\n","  wdf = pre_process_df[BM]\n","  Q3 = wdf.Weight.quantile(0.75)\n","  Q1 = wdf.Weight.quantile(0.25)\n","  IQR = Q3 - Q1\n","\n","  lower_cap = Q1-IQR*1.5\n","  upper_cap = Q3+IQR*1.5\n","\n","  BM = wdf.Weight > upper_cap\n","  pre_process_df.loc[wdf[BM].index,'Weight'] = upper_cap\n","\n","  BM = wdf.Weight < lower_cap\n","  pre_process_df.loc[wdf[BM].index,'Weight'] = lower_cap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmdFqvGMZJe9"},"outputs":[],"source":["plt.subplot(1,2,1)\n","sns.boxplot(y=pre_process_df.Height,x=pre_process_df.Gender)\n","\n","plt.subplot(1,2,2)\n","sns.boxplot(y=pre_process_df.Weight, x=pre_process_df.Gender)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"Pm5ep9XlZJe9"},"source":["#### 2.4 Detecting multivariate outliers and dealing with them\n","To detect multivariate outliers, the standard method is to use clustering analysis; however, when two of the three attributes are numerical and the other is categorical, we can do outlier detection using a specific visualization technique."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIRLhKutZJe-"},"outputs":[],"source":["cat_attribute_poss = pre_process_df.Gender.unique()\n","for i,poss in enumerate(cat_attribute_poss):\n","  BM = pre_process_df.Gender == poss\n","  pre_process_df[BM].plot.scatter(x='Height',y='Weight')\n","  plt.title(poss)\n","  plt.show()"]},{"cell_type":"markdown","source":["- Based on the preceding screenshot, we can conclude that there are **no multivariate outliers** in the data. If there were any, the only choice we would have would be to remove them, as outliers can negatively impact LR performance. Also, replacing the outliers with upper and lower caps is not an option for multivariate outliers."],"metadata":{"id":"qLY8c5XBTSYn"}},{"cell_type":"markdown","metadata":{"id":"1DMOASqLZJe-"},"source":["#### 2.5 Applying linear regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLEzkkRaZJe-"},"outputs":[],"source":["pre_process_df.Gender.replace({'male':0,'female':1},inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j84iowWcZJe-"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","X = ['Height','Gender']\n","y = 'Weight'\n","\n","data_X = pre_process_df[X]\n","data_y = pre_process_df[y]\n","\n","lm = LinearRegression()\n","lm.fit(data_X, data_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwFDE5ZyZJe-"},"outputs":[],"source":["print('intercept (b0) ', lm.intercept_)\n","coef_names = ['b1','b2']\n","print(pd.DataFrame({'Predictor': data_X.columns,\n","                    'coefficient Name':coef_names,\n","                    'coefficient Value': lm.coef_}))"]},{"cell_type":"markdown","source":["- The equation can now predict the individual Weight value based on their\n","Height and Gender values: <font color='green'>$Weight = −51.1038 + 0.7040×Height −8.6020×Gender$</font>\n","\n"],"metadata":{"id":"5lU0BrICT8ii"}},{"cell_type":"markdown","metadata":{"id":"wbegCOe0ZJe_"},"source":["## Errors\n","- Errors are an inevitable part of any data collection and measurement. The following formula best captures this fact: <font color='blue'>*Data = True Signal + Error*</font>\n","- The *True Signal* is the reality we are trying to measure and present in the form of *Data*, but due to the incapability of our measurement system or data presentation, we cannot capture the *True Signal*. Therefore, Error is the difference between the *True Signal* and the recorded *Data*.\n","\n","## Types of errors\n","1. **Systematic Errors**: Systematic errors are errors that have a clear cause and can be eliminated for future experiments\n","2. **Random Errors**: Random errors occur randomly, and sometimes have no source/cause\n","3. **Blunders**: Blunders are simply a clear mistake that causes an error in the experiment\n","\n","<font size='2'>*Ref: https://www.expii.com/t/types-of-error-overview-comparison-8112*</font>\n","\n","<img src=\"https://drive.google.com/uc?id=1tqjmQ-5ivaatPoWKatNlJgjOrVSeqQZo\" width=\"700\"/>"]},{"cell_type":"markdown","source":["## Dealing with errors\n","We will deal with errors differently based on their types. **Random errors** are unavoidable and, at best, we may be able to mitigate them using smoothing or aggregation.\n","\n","However, **systematic errors** are avoidable, and once recognized, we should always take the following steps in dealing with them:\n","1. Adjust and improve the data collection so that systematic errors will not happen in the future.\n","2. Try to use other data resources if available to find the correct value, and if there are none, we will regard the systematic error as a missing value.\n","\n","## Detecting systematic errors\n","Detecting systematic errors is not very easy, and it is likely that they go unnoticed and negatively influence our analysis. The best chance we have in detecting systematic errors is the techniques we learned in the detecting outliers section. When outliers are detected and there is no explanation why the value of the outliers are correct, then we can conclude that outliers are systematic errors.\n","\n","### Example of systematic error and correct outlier\n","In this example, we would like to analyze **Customer Entries.xlsx**. The dataset contains about 2 months of customer-visiting data from a local coffee shop between October 1, 2020, and November 24, 2020. The goal of the analysis is to profile the hours of the day to see at which times and days peak customer visits happen."],"metadata":{"id":"Cj7G-uulbFq-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8I0J5hQvZJe_"},"outputs":[],"source":["hour_df = pd.read_excel('Customer Enteries.xlsx')\n","hour_df.info()"]},{"cell_type":"code","source":["hour_df.head(5)"],"metadata":{"id":"LTBFNOUzdbyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_OAX1NCZJe_"},"outputs":[],"source":["hour_df.N_Customers.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qlIxPrMZJfA"},"outputs":[],"source":["hour_df[hour_df.N_Customers>20]"]},{"cell_type":"markdown","source":["To check whether this outlier is a case of a **systematic error** or not, we investigate using our other sources and we realize that nothing out of the ordinary had happened during that day, and this record could simply be a manual data entry error. This shows us that this is a systematic error, and therefore we need to take the following two steps in dealing with systematic errors:\n","1. *Step 1*: We inform the entity who is in charge of data collection about this mistake and ask them to take appropriate measures to prevent such a mistake from happening in the future.\n","2. *Step 2*: If we do not have ways to find the correct value using other resources within a reasonable time and effort, we regard the data entry as a missing value and replace it with **np.nan**."],"metadata":{"id":"eEf4fRIKeYWO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoezTHRCZJfA"},"outputs":[],"source":["err_index = hour_df[hour_df.N_Customers>20].index\n","hour_df.iloc[err_index,2] = np.nan"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31dzgwsUZJfA"},"outputs":[],"source":["hour_df.N_Customers.plot()\n","plt.show()"]},{"cell_type":"markdown","source":["In this dataset, time and data have already been separated, so we can perform the following bivariate outlier detection. The best way to perform bivariate outlier detection for a pair of numerical-categorical attributes is to use multiple boxplots."],"metadata":{"id":"1rcLEApenJPp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ46w24oZJfA"},"outputs":[],"source":["sns.boxplot(y=hour_df.N_Customers,x=hour_df.Time)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHyGLW0cZJfA"},"outputs":[],"source":["hour_df.query(\"Time==17 and N_Customers>12\")"]},{"cell_type":"markdown","source":["Looking at the preceding screenshot, we do see that we have two other outliers that could be systematic errors. The first one is the **smallest** value of N_Customers, which is **zero**, under the Time value of 17. The value is consistent with the rest of the data. The Time value of 17 (or 5 P.M.) seems to be getting the least number of customers, and we can imagine occasionally having no customers at that hour.\n","<br></br>\n","However, the second flier at the same hour (5 P.M.) seems more troubling. After running hour_df.query(\"Time==17 and N_Customers>12\"), which filters the flier, we can see the outlier has happened on November 17, 2020. After investigation, it turns out that on November 17, 2020 at 4:25, a biking club made a half hour stop for refreshment, which was out of the ordinary for the store. Therefore the data entry was **not erroneous** and just a **correct outlier**."],"metadata":{"id":"tT0diymqo5vF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZOq-YWeZJfA"},"outputs":[],"source":["hour_df.groupby('Time').N_Customers.median().plot.bar()\n","plt.show()"]},{"cell_type":"markdown","source":["Drawing a bar chart that shows and compares the **central tendency** of N_Customers per working hour of the coffee shop (Time) will be the visualization we need for this analysis.\n","<br></br>\n","The prescribed bar chart can easily deal with missing values as per the aggregation of the data to calculate the central tendencies. As we have outliers in the dataset, we chose to use **median** over **mean** as the central tendency for this analysis."],"metadata":{"id":"u1QXc187pjBO"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}